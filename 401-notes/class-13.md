# Readings: Linear Regressions
## Simple Linear Regression With scikit-learn
* Step 1: Import packages and classes

The first step is to import the package numpy and the class LinearRegression from sklearn.linear_model:

import numpy as np
from sklearn.linear_model import LinearRegression

* Step 2: Provide data

The second step is defining data to work with. The inputs (regressors, ð‘¥) and output (predictor, ð‘¦) should be arrays (the instances of the class numpy.ndarray) or similar objects. This is the simplest way of providing data for regression:

x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
y = np.array([5, 20, 14, 32, 22, 38])

* Step 3: Create a model and fit it

model = LinearRegression()

    - fit_intercept is a Boolean (True by default) that decides whether to calculate the intercept ð‘â‚€ (True) or consider it equal to zero (False).
    - normalize is a Boolean (False by default) that decides whether to normalize the input variables (True) or not (False).
    - copy_X is a Boolean (True by default) that decides whether to copy (True) or overwrite the input variables (False).
    - n_jobs is an integer or None (default) and represents the number of jobs used in parallel computation. None usually means one job and -1 to use all processors.

Itâ€™s time to start using the model. First, you need to call .fit() on model:

* model.fit(x, y)

With .fit(), you calculate the optimal values of the weights ð‘â‚€ and ð‘â‚, using the existing input and output (x and y) as the arguments. In other words, .fit() fits the model. It returns self, which is the variable model itself. Thatâ€™s why you can replace the last two statements with this one:

model = LinearRegression().fit(x, y)
This statement does the same thing as the previous two. Itâ€™s just shorter.

* Step 4: Get results
Once you have your model fitted, you can get the results to check whether the model works satisfactorily and interpret it.

You can obtain the coefficient of determination (ð‘…Â²) with .score() called on model:

>>> r_sq = model.score(x, y)
>>> print('coefficient of determination:', r_sq)
coefficient of determination: 0.715875613747954

When youâ€™re applying .score(), the arguments are also the predictor x and regressor y, and the return value is ð‘…Â².

The attributes of model are .intercept_, which represents the coefficient, ð‘â‚€ and .coef_, which represents ð‘â‚:

>>> print('intercept:', model.intercept_)
intercept: 5.633333333333329
>>> print('slope:', model.coef_)
slope: [0.54]

* Step 5: Predict response

Once there is a satisfactory model, you can use it for predictions with either existing or new data.

To obtain the predicted response, use .predict():

>>> y_pred = model.predict(x)
>>> print('predicted response:', y_pred, sep='\n')
predicted response:
[ 8.33333333 13.73333333 19.13333333 24.53333333 29.93333333 35.33333333]

When applying .predict(), you pass the regressor as the argument and get the corresponding predicted response.

This is a nearly identical way to predict the response:

>>> y_pred = model.intercept_ + model.coef_ * x
>>> print('predicted response:', y_pred, sep='\n')
predicted response:
[[ 8.33333333]
 [13.73333333]
 [19.13333333]
 [24.53333333]
 [29.93333333]
 [35.33333333]]

## Multiple Linear Regression With scikit-learn

* Steps 1 and 2: Import packages and classes, and provide data
import numpy as np
from sklearn.linear_model import LinearRegression

x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]
y = [4, 5, 20, 14, 32, 22, 38, 43]
x, y = np.array(x), np.array(y)

* Step 3: Create a model and fit it
model = LinearRegression().fit(x, y)

* Step 4: Get results

You can obtain the properties of the model the same way as in the case of simple linear regression:

>>> r_sq = model.score(x, y)
>>> print('coefficient of determination:', r_sq)
coefficient of determination: 0.8615939258756776
>>> print('intercept:', model.intercept_)
intercept: 5.52257927519819
>>> print('slope:', model.coef_)
slope: [0.44706965 0.25502548]

* Step 5: Predict response

Predictions also work the same way as in the case of simple linear regression:

>>> y_pred = model.predict(x)
>>> print('predicted response:', y_pred, sep='\n')
predicted response:
[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957
 38.78227633 41.27265006]

The predicted response is obtained with .predict(), which is very similar to the following:

>>> y_pred = model.intercept_ + np.sum(model.coef_ * x, axis=1)
>>> print('predicted response:', y_pred, sep='\n')
predicted response:
[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957
 38.78227633 41.27265006]

Fortunately, there are other regression techniques suitable for the cases where linear regression doesnâ€™t work well. Some of them are support vector machines, decision trees, random forest, and neural networks.

## Train/Test Split
As I said before, the data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our modelâ€™s prediction on this subset.

Letâ€™s see how to do this in Python. Weâ€™ll do this using the Scikit-Learn library and specifically the train_test_split method. Weâ€™ll start with importing the necessary libraries:
* import pandas as pd
* from sklearn import datasets, linear_model
* from sklearn.model_selection import train_test_split
* from matplotlib import pyplot as plt